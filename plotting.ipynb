{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Visualizing Attention Maps",
   "id": "2ee5438fb0d066e8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from src.models import ViTBinaryClassifier\n",
    "from src.dataset import ImagingDataset\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from monai.networks.nets import ViTAutoEnc\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- Configuration ---\n",
    "folds = 5\n",
    "trial = 50\n",
    "weights_dir = \"weights\"\n",
    "img_seq_path = \"imgs/molab-hardy-leaf-97_images.npy\"\n",
    "test_csv = \"dataframes/threshold_df_new.csv\"\n",
    "curated_csv = \"dataframes/molab_df_curated.csv\"\n",
    "label_col = 'label-1RN-0Normal'\n",
    "exclude_columns = ['label-1RN-0Normal', 'Patient ID', 'id', 'BASELINE_TIME_POINT', \"CROSSING_TIME_POINT\", \"BASELINE_VOLUME\", \"scan_date\"]\n",
    "save_dir = r\"C:\\Users\\gomaaad\\PycharmProjects\\attn_maps\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load test dataframe\n",
    "geo_df = pd.read_csv(test_csv)\n",
    "curated_df = pd.read_csv(curated_csv)\n",
    "geo_df = geo_df.merge(curated_df[['Patient ID', 'id', label_col]], on=['Patient ID', 'id'], how='left')\n",
    "geo_df = geo_df[geo_df[label_col].notna()]\n",
    "\n",
    "# Prepare dataset and dataloader for a few samples\n",
    "ds = ImagingDataset(geo_df, data_dir=img_seq_path, is_gap=False, is_img=True)\n",
    "sample_loader = torch.utils.data.DataLoader(ds, batch_size=1, shuffle=False)\n",
    "\n",
    "# Load pretrained ViT model\n",
    "pretrained_model_path = \"C:/Users/gomaaad/PycharmProjects/pretrainedmodels/hardy-leaf-97/best_model_942_0.291.pt\"  # Update with actual path\n",
    "pre_trained_model = ViTAutoEnc(\n",
    "    img_size=(64, 64, 64),\n",
    "    patch_size=8,\n",
    "    in_channels=1,\n",
    "    out_channels=1,\n",
    "    num_layers=12,\n",
    "    num_heads=12,\n",
    "    hidden_size=384,\n",
    "    mlp_dim=2048\n",
    ")\n",
    "state_dict = torch.load(pretrained_model_path, map_location=\"cpu\", weights_only=False)\n",
    "if any(k.startswith(\"module.\") for k in state_dict.keys()):\n",
    "    state_dict = {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}\n",
    "pre_trained_model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "\n",
    "# Load ViTBinaryClassifier and weights\n",
    "model = ViTBinaryClassifier(pretrained_model=pre_trained_model, unfreeze_last_n=0)\n",
    "weight_path = os.path.join(weights_dir, \"imaging\",f\"model_fold_{4}_trial_{trial}.pth\")  # Adjust trial number if needed\n",
    "if not os.path.exists(weight_path):\n",
    "    print(f\"Weight file not found: {weight_path}\")\n",
    "model.load_state_dict(torch.load(weight_path, map_location=\"cpu\"))\n",
    "model.eval()\n",
    "\n",
    "# Get a sample and forward pass\n",
    "for i, (x, _) in enumerate(sample_loader):\n",
    "    # x -> [1, 1, 64, 64, 64]\n",
    "    with torch.no_grad():\n",
    "        # Forward pass\n",
    "        output = model(x)\n",
    "\n",
    "        # Extract attention weights from last transformer block\n",
    "        last_block = model.blocks[-1]\n",
    "        attn_weights = last_block.attn.att_mat  # [1, 12, N, N]\n",
    "\n",
    "        # Average over heads and keep only attention from each patch to itself or to others\n",
    "        attn_weights = attn_weights.mean(dim=1).squeeze(0)  # [N, N]\n",
    "\n",
    "        # Take mean attention given *to* each patch\n",
    "        token_attn = attn_weights.mean(dim=0)  # [N] -- attention received per patch\n",
    "\n",
    "        # Reshape attention to 3D grid (assuming patch size = 8 for 64³ volume → 8x8x8 grid)\n",
    "        num_patches_per_dim = x.shape[-1] // 8  # = 8\n",
    "        attn_map = token_attn.reshape(num_patches_per_dim, num_patches_per_dim, num_patches_per_dim)\n",
    "\n",
    "        # Upsample attention map to original resolution (64x64x64)\n",
    "        attn_map = attn_map.unsqueeze(0).unsqueeze(0)  # [1,1,8,8,8]\n",
    "        attn_map_upsampled = F.interpolate(attn_map, size=(64, 64, 64), mode='trilinear', align_corners=False)\n",
    "        attn_map_upsampled = attn_map_upsampled.squeeze().cpu().numpy()  # [64, 64, 64]\n",
    "\n",
    "        # Visualize a few axial slices\n",
    "        input_volume = x.squeeze().cpu().numpy()  # [64, 64, 64]\n",
    "\n",
    "        # Directory to save figures\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "        print(f\"Saving attention map for sample {i}\")\n",
    "        fig, ax = plt.subplots(figsize=(5, 5))\n",
    "        ax.imshow(input_volume[:, :, 31], cmap='gray', alpha=0.8)\n",
    "        ax.imshow(attn_map_upsampled[:, :, 31], cmap='jet', alpha=0.4)\n",
    "        ax.axis('off')\n",
    "\n",
    "        # Save the figure as PDF without title\n",
    "        destination = os.path.join(save_dir, f\"attention_map_sample_{i}_slice_{31}.pdf\")\n",
    "        fig.savefig(destination, bbox_inches='tight', pad_inches=0)\n",
    "        plt.show()\n",
    "        plt.close(fig)  # Close to avoid memory buildup\n",
    "\n",
    "\n"
   ],
   "id": "df4d279d640dd985",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "bff4e1c0868d86ab",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
